# -*- coding: utf-8 -*-
"""chatbot-299.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R__rj04yOoAhy_bfTsDfZqPjiKKlncZv
"""

!pip install -q numpy pandas matplotlib transformers PyPDF2 langchain-community \
                 faiss-cpu tiktoken pymupdf sentence-transformers langchain-huggingface

!pip install groq

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from langchain.text_splitter import RecursiveCharacterTextSplitter

import fitz
import os

# Directory where your PDFs are stored
pdf_directory = '/content/'

# Get all PDF files in the directory
pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]

# Process each PDF
for pdf_file in pdf_files:
    pdf_path = os.path.join(pdf_directory, pdf_file)
    print(f"\nProcessing: {pdf_file}\n{'-'*40}")

    # Open and extract text
    doc = fitz.open(pdf_path)
    full_text = "\n".join([page.get_text() for page in doc])
    doc.close()

    # Print the first 500 characters as preview
    print(full_text[:500] + ("..." if len(full_text) > 500 else ""))
    print(f"\nExtracted {len(full_text)} characters from {pdf_file}")

print(f"\nDone! Processed {len(pdf_files)} PDF files")

import fitz
import os
import tiktoken
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. Setup
pdf_directory = '/content/'
encoder = tiktoken.encoding_for_model("gpt-4")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=lambda x: len(encoder.encode(x)),
)

# 2. Process PDFs
pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]
all_chunks = []

for pdf_file in pdf_files:
    pdf_path = os.path.join(pdf_directory, pdf_file)

    # Extract text
    doc = fitz.open(pdf_path)
    full_text = "\n".join([page.get_text() for page in doc])
    doc.close()

    # Create chunks WITHIN the loop
    chunks = text_splitter.create_documents(
        [full_text],
        metadatas=[{"source": pdf_path}]  # or pdf_file for just the name
    )
    all_chunks.extend(chunks)

    print(f"Created {len(chunks)} chunks from {pdf_file}")

# 3. Final output
print(f"\nTotal: {len(all_chunks)} chunks from {len(pdf_files)} PDFs")

from langchain_huggingface import HuggingFaceEmbeddings
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': False}
)

from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# 4. Create FAISS Vector Store
db = FAISS.from_documents(
    documents=all_chunks,
    embedding=embedding_model
)

# 5. Save and Create Retriever
db.save_local("faiss_db")
print("FAISS index saved successfully")

retriever = db.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

# 6. Query the Retriever
query = "What is Digital Security Act?"
results = retriever.invoke(query)
print(results)

# Print results
for i, doc in enumerate(results, 1):
    print(f"\nResult {i}:")
    print(doc.page_content)
    print(f"Source: {doc.metadata.get('source')}")

from groq import Groq
from langchain.llms.base import LLM
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from typing import Any

# initialize groq client
client = Groq(api_key="GROQ_API_KEY")

class GroqLLM(LLM):
  groq_client: Any = None

  def __init__(self, groq_client: Any, **kwargs: Any):
    super().__init__(**kwargs)
    if groq_client:
      self.groq_client = groq_client

  def _call(self, prompt, stop = None):
    response = self.groq_client.chat.completions.create(
        messages = [{"role": "user", "content": prompt}],
        model = "llama-3.1-8b-instant",
        max_tokens = 256,
        temperature = 0.5
    )
    return response.choices[0].message.content

  @property
  def _llm_type(self):
    return "groq"

llm = GroqLLM(client)

from langchain_core.prompts import PromptTemplate

CUSTOM_PROMPT = """You are a legal expert specializing in every laws in Bangladesh. Answer the question using ONLY the provided context. If unsure, say "I cannot find a definitive answer."

Follow these rules:
1. Be precise and cite relevant sections when possible
2. Use bullet points for multi-part answers
3. Keep responses under 150 words
4. Never invent information

Context: {context}
Question: {question}

Answer in this format:
**Summary**: [1-2 sentence overview]
**Details**:
- [Key point 1 with section reference if available]
- [Key point 2]
**Source**: [document name]"""

prompt = PromptTemplate(
    input_variables = ["context", "question"],
    template = CUSTOM_PROMPT
)

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
db = FAISS.load_local("faiss_db", embedding_model, allow_dangerous_deserialization=True)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")

chain = ConversationalRetrievalChain.from_llm(
    llm = llm,
    retriever = retriever,
    memory = memory,
    return_source_documents = True,
    combine_docs_chain_kwargs = {"prompt": prompt}
)
print("Conversational Retrieval Chain loaded successfully")

test_queries = [
    "What is Digital Security Act?",
    "What are penalties mentioned in the act?",
    "How to file a domestic violence complaint?"
]

for query in test_queries:
    result = chain({"question": query})
    print(result['answer'])

def summarize_pdf(pdf_file):
    """Extract text from PDF and create summary (using 1st code's logic with 2nd code's variables)"""
    # Extract text from PDF (using same method as your existing code)
    doc = fitz.open(pdf_file.name)
    full_text = "\n".join([page.get_text() for page in doc])
    doc.close()

    if not full_text.strip():
        return "Error: No text could be extracted from the PDF. It might be empty, scanned, or protected."

    print(f"Extracted text from {pdf_file.name} ({len(full_text)} characters)")

    encoded = encoder.encode(full_text)
    max_tokens = 6000

    if len(encoded) > max_tokens:
        print(f"Document is large ({len(encoded)} tokens). Processing in chunks...")

        # Split into chunks for summarization
        chunks = text_splitter.create_documents([full_text], metadatas=[{"source": pdf_file.name}])

        # Summarize each chunk
        chunk_summaries = []
        for i, chunk in enumerate(chunks[:10]):  # Limit to first 10 chunks to avoid excessive processing
            chunk_prompt = f"""Summarize the following text section concisely, focusing on key points:

{chunk.page_content}

Provide a brief summary (2-3 sentences):"""

            summary = llm._call(chunk_prompt)
            chunk_summaries.append(summary)

            if (i + 1) % 3 == 0:
                print(f"Processed {i + 1} chunks...")

        # Combine chunk summaries
        combined = "\n\n".join(chunk_summaries)

        final_prompt = f"""Create a comprehensive summary of this document based on the following section summaries:

{combined}

Provide a well-structured summary covering:
1. Main topic and purpose
2. Key points and findings
3. Important conclusions or recommendations

Summary:"""

    else:
        final_prompt = f"""Please provide a comprehensive summary of the following document:

{full_text}

Create a well-structured summary covering:
1. Main topic and purpose
2. Key points and findings
3. Important conclusions or recommendations

Summary:"""

    # Generate final summary
    final_summary = llm._call(final_prompt)
    return final_summary

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Specify your case PDF file name here
CASE_PDF_NAME = "Legal Cases in Bangladesh.pdf"

# Extract only case texts from your specific case PDF
case_texts = []
case_sources = []

for chunk in all_chunks:
    # Check if this chunk is from the case PDF
    if CASE_PDF_NAME in chunk.metadata['source']:
        case_texts.append(chunk.page_content)
        case_sources.append(chunk.metadata['source'])

print(f"Loaded {len(case_texts)} case chunks from {CASE_PDF_NAME} for similarity search")

# Create TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(
    max_features=5000,
    stop_words='english',
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.8
)

# Fit and transform the case texts
tfidf_matrix = tfidf_vectorizer.fit_transform(case_texts)
print("TF-IDF matrix created successfully")

# Function to find similar cases with FULL TEXT
def find_similar_cases(query_text, top_k=5):
    # Transform query using the same vectorizer
    query_tfidf = tfidf_vectorizer.transform([query_text])

    # Calculate cosine similarity
    similarity_scores = cosine_similarity(query_tfidf, tfidf_matrix).flatten()

    # Get top similar cases
    top_indices = similarity_scores.argsort()[-top_k:][::-1]

    similar_cases = []
    for idx in top_indices:
        similar_cases.append({
            'text': case_texts[idx],
            'source': case_sources[idx],
            'similarity_score': similarity_scores[idx]
        })

    return similar_cases

# Test the case similarity search
test_case_queries = [
    "domestic violence case legal precedent",
    "property dispute inheritance law",
    "contract breach commercial law"
]

for query in test_case_queries:
    print(f"\nQuery: {query}")
    print("-" * 50)

    similar_cases = find_similar_cases(query, top_k=3)

    for i, case in enumerate(similar_cases, 1):
        print(f"Text Preview: {case['text']}")
        print("-" * 30)

print("\nCase Similarity Search feature loaded successfully!")

import gradio as gr

# Legal Chatbot
def legal_chatbot(message, history):
    result = chain({"question": message})
    # Add the new message and response to history
    history.append([message, result['answer']])
    return history, ""

# PDF Summarizer
def pdf_summarizer(pdf_file):
    if pdf_file is None:
        return "Please upload a PDF file."

    try:
        summary = summarize_pdf(pdf_file)
        return summary
    except Exception as e:
        return f"Error processing PDF: {str(e)}"

# Case Similarity Search function
def case_search(query_text):
    if not query_text.strip():
        return "Please enter a search query."

    similar_cases = find_similar_cases(query_text, top_k=3)

    if not similar_cases:
        return "No similar cases found."

    result = f"**Similar Cases for: '{query_text}'**\n\n"

    for i, case in enumerate(similar_cases, 1):
        result += f"**Case {i}:**\n"
        result += f"{case['text']}\n"
        result += "-" * 80 + "\n\n"

    return result

# Create Gradio interface
with gr.Blocks(title="AI Legal Aid System for Bangladesh", theme=gr.themes.Soft()) as demo:

    gr.Markdown("# 🏛️ AI-Powered Legal Aid for Bangladesh")
    gr.Markdown("Get legal assistance, document summaries, and find similar cases using AI technology.")

    with gr.Tabs():

        # Tab 1: Legal Chatbot
        with gr.TabItem("💬 Legal Chatbot"):
            gr.Markdown("### Ask any legal question about Bangladesh law")

            chatbot = gr.Chatbot(
                height=400,
                placeholder="Legal AI Assistant will appear here..."
            )

            msg = gr.Textbox(
                placeholder="Type your legal question here... (e.g., 'What is Digital Security Act?')",
                label="Your Question"
            )

            clear = gr.Button("Clear Chat")

            msg.submit(legal_chatbot, [msg, chatbot], [chatbot, msg])
            clear.click(lambda: ([], ""), None, [chatbot, msg])

        # Tab 2: PDF Document Summarizer
        with gr.TabItem("📄 Document Summarizer"):
            gr.Markdown("### Upload a PDF to get an AI-generated summary")

            with gr.Row():
                with gr.Column():
                    pdf_input = gr.File(
                        label="Upload PDF Document",
                        file_types=[".pdf"],
                        type="filepath"
                    )

                    summarize_btn = gr.Button("Generate Summary", variant="primary")

                with gr.Column():
                    summary_output = gr.Textbox(
                        label="Document Summary",
                        lines=15,
                        placeholder="Upload a PDF and click 'Generate Summary' to see the AI-generated summary here..."
                    )

            summarize_btn.click(
                pdf_summarizer,
                inputs=pdf_input,
                outputs=summary_output
            )

        # Tab 3: Case Similarity Search
        with gr.TabItem("⚖️ Case Similarity Search"):
            gr.Markdown("### Find similar legal cases and precedents")

            with gr.Row():
                with gr.Column():
                    case_query = gr.Textbox(
                        label="Search Query",
                        placeholder="Enter legal terms or case description (e.g., 'domestic violence case')",
                        lines=3
                    )

                    search_btn = gr.Button("Search Similar Cases", variant="primary")

                with gr.Column():
                    case_results = gr.Textbox(
                        label="Similar Cases Found",
                        lines=15,
                        placeholder="Enter a search query and click 'Search Similar Cases' to find relevant legal precedents..."
                    )

            search_btn.click(
                case_search,
                inputs=case_query,
                outputs=case_results
            )

    # Footer
    gr.Markdown("---")
    gr.Markdown("**Note:** This AI system is designed to assist with legal information. Always consult with qualified legal professionals for official legal advice.")

# Launch the interface
demo.launch(debug=True, share=True)